# R Environment and Package Ecosystem {#sec-indexing}

## Why R for Data Visualization?

R is considered to be one of the most powerful tools and has a gained significant audience for a variety of factors ranging from its
flexibility to rich package ecosystem to built-in integration of statistical computing. It's features also include very efficient data
handling and storage facilities. R boasts a comprehensive ecosystem of packages designed specifically for data visualization.R is also open
sourced and encompasses a large and active community of both users and developers. R also integrates quite well with other programming
languages like Python and SQL. it also aloows for embedding of R outputs in web applications through frameworks like ,making it easier to share
visualizations in interactive web dashboards or other software applications.In academic and professional settings, the ability to
reproduce analyses is crucial. R scripts can be shared and rerun, providing an auditable trail of how visualizations were created. This is
particularly important for transparency in research and reporting and lastly being an open-source program platform ,R is run without any
cost.This makes it a cost-effective solution for data visualization, as there are no licensing fees involved, making it accessible for students,
researchers, and businesses alike.

### Core Features of R We Utilized

### Overview of Key Packages

-   **Data Preparation:** Raw datasets were received in various formats.
    those datasets were imported by `readxl`which has no external
    dependencies making it easy to install and use on all operating
    systems. Main transformations were handled by `dplyr`using its
    **mutate()**, **filter()**, **summarize()** functions to keep the
    dataset concise. Reshaping and handling of missing data was handled
    by `tdyr`. when datasets grew bigger a switch was made to
    `data.table` to prioritize speed and memory efficency. Combining
    functions such as **fread()**, **fwrite()**, impacted the speed of
    operation greatly.[BURAYA DAHA EKLENİR]. Meanwhile `arrow` allows R
    to read and write columnar Parquet/ Feather files with minimal
    memory use loading only the columns requested through a
    `dplyr`compatible interface. This provides compression,
    multi-threaded I/O, and seamless interoperability with Python,
    Spark, and cloud storage all without changing the analyst’s R
    syntax.

-   **Exploratory data analysis:** Base summary functions and **apply
    family** were mainly used `lapply`, `sapply`, and `vapply`to name a
    few to deliver loop free computations to improve speed across
    multiple variables.

-   **Static and multivariate visualisation:** all fixed graphics were
    produced by `ggplot2`its grammer of grpahics and facet system
    covered bar charts, histograms, boxplots, scatterplots, heatmaps,
    and pair plots.

-   **Spatial Analytics:** Vector geometries were read and manipulated
    using `sf` package which stores points, lines and polygons as
    list-columns inside asn ordinary data frame. since geometries behave
    like any other columns they can be filtered, mutated and joined with
    `dplyr`. The package `tmap` added on to the `sf`package to produce
    thematic maps with a layer syntax resembling the `ggplot2`package.
    **tm_Shape()** identifies the spatial object, **tm_fill()**

-   **Interactive Reporting:** `plotly` converted ggplot objects into
    zoomable, hover-enabled widgets. Full dashboards were assembled in
    Shiny, whose reactive model lets user inputs update outputs without
    additional JavaScript. When data lived on a Spark cluster,
    `sparklyr` exposed the same `dplyr` verbs, enabling Shiny panels to
    query distributed storage on demand.

Together, these packages provide a seamless toolchain: data flow from ingestion and cleaning through exploratory statistics, static and
interactive graphics, spatial mapping, and finally to a web dashboard—all within a single, reproducible R environment.

## Integration with Big Data Tools

To meet two opposing requirements—interactive analysis on a laptop and batch processing over terabyte-scale tables—this project employs a
tiered architecture: Hadoop supplies reliable cluster storage and coarse-grain cleansing, Arrow delivers high-throughput columnar
interchange to desktop R, and SparkR provides distributed, in-memory analytics when data volumes exceed local resources. Each layer therefore
addresses a specific constraint (fault tolerance, I/O bandwidth, or compute parallelism) while exposing an R interface that preserves a
single, reproducible code base.

-   **SparkR for In-Memory Distributed Analytics** When full data
    volumes exceed workstation resources, the pipeline attaches to the
    institutional Apache Spark cluster through SparkR. Parquet files
    produced under Arrow are registered as Spark DataFrames; the
    familiar dplyr verbs are translated into Spark SQL and executed in
    parallel across worker nodes. Model artefacts needed for
    visualisation return to the driver as compact R objects, whereas
    voluminous prediction tables remain distributed on the cluster for
    downstream consumption.

By uniting Hadoop’s fault-tolerant storage, Arrow’s high-throughput columnar access, and SparkR’s distributed in-memory processing, the same
R codebase scales effortlessly from laptop-sized prototypes to terabyte-level production workloads, offering rapid iteration in
development and robust performance in deployment.

## Main R Environment Setup